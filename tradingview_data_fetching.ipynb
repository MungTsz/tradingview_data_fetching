{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "import yaml\n",
    "import json\n",
    "import holidays\n",
    "import pendulum\n",
    "import tradingview_ta\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "from cloudpathlib import S3Path\n",
    "from result import Err, Ok, is_err\n",
    "from datetime import datetime, timedelta\n",
    "from utils.logger import configure_logger\n",
    "from dateutil.relativedelta import relativedelta, MO\n",
    "from utils.constants import pair_list, interval_list\n",
    "from utils.delete_local_data_and_log_from_logger import (\n",
    "    delete_local_data,\n",
    "    delete_all_local_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_name_str = input(\"What is the name of the config file?\")\n",
    "if config_file_name_str == \"\":\n",
    "    config_file_name_str = \"config.yaml\"\n",
    "\n",
    "with open(f\"./config/{config_file_name_str}\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_local_logger(local_log_path_name: str, logger_name: str):\n",
    "    datetime_now = pendulum.now(\"Asia/Hong_Kong\").strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    configure_logger(local_log_path_name, f\"{logger_name}_{datetime_now}\")\n",
    "    logger.success(\"Logger configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_pair(target_pair_list: list) -> Ok[list] | Err[str]:\n",
    "    try:\n",
    "        if target_pair_list[0] == \"ALL\":\n",
    "            target_pair_list = [\"FX_IDC:\" + pair for pair in pair_list]\n",
    "            return Ok(target_pair_list)\n",
    "        elif target_pair_list[0] != \"ALL\":\n",
    "            # Check if all elements in target_pair_list are in pair_list list, avoid misspelling\n",
    "            is_all_in_pair_list = all(item in pair_list for item in target_pair_list)\n",
    "            unmatched_items = [\n",
    "                item for item in target_pair_list if item not in pair_list\n",
    "            ]\n",
    "            if is_all_in_pair_list is False:\n",
    "                return Err(\n",
    "                    f\"Error in configuring pair, unmatched items found: {unmatched_items}\"\n",
    "                )\n",
    "            target_pair_list = [\"FX_IDC:\" + pair for pair in target_pair_list]\n",
    "            return Ok(target_pair_list)\n",
    "    except Exception as e:\n",
    "        return Err(f\"Error in configuring pair: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_interval(target_interval_list: list) -> Ok[list] | Err[str]:\n",
    "    try:\n",
    "        if target_interval_list[0] == \"ALL\":\n",
    "            target_interval_list = interval_list\n",
    "            return Ok(target_interval_list)\n",
    "        elif target_interval_list[0] != \"ALL\":\n",
    "            # Check if all elements in interval_list are in interval list, avoid misspelling\n",
    "            is_all_in_interval_list = all(\n",
    "                item in interval_list for item in target_interval_list\n",
    "            )\n",
    "            unmatched_items = [\n",
    "                item for item in target_interval_list if item not in interval_list\n",
    "            ]\n",
    "            if is_all_in_interval_list is False:\n",
    "                return Err(\n",
    "                    f\"Error in configuring time_interval, unmatched items found: {unmatched_items}\"\n",
    "                )\n",
    "            target_interval_list = target_interval_list\n",
    "            return Ok(target_interval_list)\n",
    "    except Exception as e:\n",
    "        return Err(f\"Error in configuring time_interval: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_handler(target_interval_list: list) -> Ok[dict] | Err[str]:\n",
    "    israel_timezone = pytz.timezone(\"Israel\")\n",
    "    israel_now = datetime.now(israel_timezone)\n",
    "    current_minute = israel_now.minute\n",
    "    current_hour = israel_now.hour\n",
    "    current_date = israel_now.date()\n",
    "\n",
    "    country_code = \"US\"\n",
    "    public_holidays = holidays.CountryHoliday(country_code)\n",
    "\n",
    "    datetime_dict = {}\n",
    "    try:\n",
    "        for interval in target_interval_list:\n",
    "            if interval.endswith(\"m\"):\n",
    "                interval_value = int(interval[:-1])\n",
    "                rounded_value = (current_minute // interval_value) * interval_value\n",
    "                # Format the rounded value with leading zeros if necessary\n",
    "                formatted_rounded_value = \"{:02d}\".format(rounded_value)\n",
    "                formatted_datetime = israel_now.strftime(\n",
    "                    \"%Y-%m-%d-%H-{}-00\".format(formatted_rounded_value)\n",
    "                )\n",
    "                datetime_dict[interval] = formatted_datetime\n",
    "\n",
    "            elif interval.endswith(\"h\"):\n",
    "                interval_value = int(interval[:-1])\n",
    "                rounded_value = (current_hour // interval_value) * interval_value\n",
    "                formatted_rounded_value = \"{:02d}\".format(rounded_value)\n",
    "                formatted_datetime = israel_now.strftime(\n",
    "                    \"%Y-%m-%d-{}-00-00\".format(formatted_rounded_value)\n",
    "                )\n",
    "                datetime_dict[interval] = formatted_datetime\n",
    "\n",
    "            elif interval.endswith(\"d\"):\n",
    "                while current_date in public_holidays:\n",
    "                    current_date += timedelta(days=1)\n",
    "                formatted_datetime = \"{}-00-00-00\".format(current_date)\n",
    "                datetime_dict[interval] = formatted_datetime\n",
    "\n",
    "            elif interval.endswith(\"W\"):\n",
    "                # Pass MO(-1) as an argument to relativedelta to set weekday as Monday and -1 signifies last week's Monday\n",
    "                current_monday = current_date + relativedelta(weekday=MO(-1))\n",
    "                # Check if the Monday date is international public holiday (i.e. US)\n",
    "                while current_monday in public_holidays:\n",
    "                    current_monday += timedelta(days=1)\n",
    "                formatted_datetime = \"{}-00-00-00\".format(current_monday)\n",
    "                datetime_dict[interval] = formatted_datetime\n",
    "\n",
    "            elif interval.endswith(\"M\"):\n",
    "                first_day_of_current_month = current_date + relativedelta(day=1)\n",
    "                while first_day_of_current_month in public_holidays:\n",
    "                    first_day_of_current_month += timedelta(days=1)\n",
    "                formatted_datetime = \"{}-00-00-00\".format(first_day_of_current_month)\n",
    "                datetime_dict[interval] = formatted_datetime\n",
    "        return Ok(datetime_dict)\n",
    "\n",
    "    except Exception as e:\n",
    "        return Err(f\"Error when handling the datetime: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_technical_analysis_summary(\n",
    "    screener: str, target_interval_list: list, target_pair_list: list\n",
    ") -> Ok[dict] | Err[str]:\n",
    "    try:\n",
    "        # add each pair as a key to the dictionary with an empty dictionary first\n",
    "        technical_analysis_summary_dict = {}\n",
    "        for tradingview_symbol in target_pair_list:\n",
    "            # e.g. extract \"AUDCAD\" from \"FX_IDC:AUDCHF\" for the key of the dict\n",
    "            pair = tradingview_symbol.split(\":\")[1]\n",
    "            technical_analysis_summary_dict[pair] = {}\n",
    "\n",
    "        # for interval and pair, fetch data\n",
    "        for interval in target_interval_list:\n",
    "            technical_analysis_summary = tradingview_ta.get_multiple_analysis(\n",
    "                screener=screener, interval=interval, symbols=target_pair_list\n",
    "            )\n",
    "            for tradingview_symbol in target_pair_list:\n",
    "                pair = tradingview_symbol.split(\":\")[1]\n",
    "                technical_analysis_summary_dict[pair].update(\n",
    "                    {interval: technical_analysis_summary[tradingview_symbol].summary}\n",
    "                )\n",
    "        return Ok(technical_analysis_summary_dict)\n",
    "    except Exception as e:\n",
    "        return Err(f\"Error in fetching technical analysis summary: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_technical_indicators(\n",
    "    screener: str, target_interval_list: list, target_pair_list: list\n",
    ") -> Ok[dict] | Err[str]:\n",
    "    try:\n",
    "        technical_indicators_dict = {}\n",
    "        for tradingview_symbol in target_pair_list:\n",
    "            # e.g. extract \"AUDCAD\" from \"FX_IDC:AUDCHF\" for the key of the dict\n",
    "            pair = tradingview_symbol.split(\":\")[1]\n",
    "            technical_indicators_dict[pair] = {}\n",
    "\n",
    "        #  for interval and pair, fetch data\n",
    "        for interval in target_interval_list:\n",
    "            technical_indicators = tradingview_ta.get_multiple_analysis(\n",
    "                screener=screener, interval=interval, symbols=target_pair_list\n",
    "            )\n",
    "            for tradingview_symbol in target_pair_list:\n",
    "                # e.g. extract \"AUDCAD\" from \"FX_IDC:AUDCHF\" for the key of the dict\n",
    "                pair = tradingview_symbol.split(\":\")[1]\n",
    "                technical_indicators_dict[pair].update(\n",
    "                    {interval: technical_indicators[tradingview_symbol].indicators}\n",
    "                )\n",
    "        return Ok(technical_indicators_dict)\n",
    "    except Exception as e:\n",
    "        return Err(f\"Error in fetching technical analysis summary: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_dict(\n",
    "    technical_analysis_summary: dict, technical_indicators: dict\n",
    ") -> dict:\n",
    "    dict1 = technical_analysis_summary\n",
    "    dict2 = technical_indicators\n",
    "    combined_dict = dict1\n",
    "    for outer_pair_key, outer_dict_value in dict2.items():\n",
    "        for inner_interval_key, inner_dict_value in outer_dict_value.items():\n",
    "            for key, value in inner_dict_value.items():\n",
    "                combined_dict[outer_pair_key][inner_interval_key][key] = value\n",
    "    return combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_as_json_to_local_file(\n",
    "    combined_dict: dict, local_data_base_path: Path, datetime_dict: dict\n",
    ") -> Ok[str] | Err[str]:\n",
    "    for outer_pair_key, outer_dict_value in combined_dict.items():\n",
    "        for inner_interval_key, inner_dict_value in outer_dict_value.items():\n",
    "            local_output_path = local_data_base_path / outer_pair_key\n",
    "            local_output_path.mkdir(parents=True, exist_ok=True)\n",
    "            file_name = f\"{outer_pair_key}_{inner_interval_key}_{datetime_dict[inner_interval_key]}.json\"\n",
    "            local_file_path = local_output_path / file_name\n",
    "\n",
    "            with open(local_file_path, \"w\") as json_file:\n",
    "                json.dump(\n",
    "                    {outer_pair_key: {inner_interval_key: inner_dict_value}}, json_file\n",
    "                )\n",
    "            if not local_file_path.exists():\n",
    "                return Err(f\"{local_file_path} does not exist\")\n",
    "\n",
    "    return Ok(\"All data was outputted to JSON files and saved in the local folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_key(filename: str) -> str:\n",
    "    pair, interval, date = filename.split(\"_\")\n",
    "    year, month, day, hour, minute, second = date.split(\"-\")\n",
    "    return f\"{interval}/{year}/{month}/{day}/{hour}/{filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_local_data_to_s3(\n",
    "    target_pair_list: list,\n",
    "    local_data_base_path: Path,\n",
    "    s3_output_base_path: S3Path,\n",
    ") -> Ok[str] | Err[str]:\n",
    "    try:\n",
    "        for tradingview_symbol in target_pair_list:\n",
    "            pair = tradingview_symbol.split(\":\")[1]\n",
    "            local_file_path = local_data_base_path / pair\n",
    "            for path in local_file_path.glob(\"**/*.json\"):\n",
    "                filename = path.name\n",
    "                s3_key = generate_key(filename)\n",
    "                s3_output_data_path = s3_output_base_path / pair / s3_key\n",
    "                s3_output_data_path.upload_from(path)\n",
    "                logger.success(\n",
    "                    f\"Successfully uploaded {local_file_path} to {s3_output_data_path}\"\n",
    "                )\n",
    "    except Exception as e:\n",
    "        return Err(f\"Error uploading {local_file_path} to {s3_output_data_path}: {e}\")\n",
    "    return Ok(\"Finish uploading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_latest_log_to_s3(\n",
    "    local_log_path: Path, s3_output_base_path: S3Path\n",
    ") -> Ok[str] | Err[str]:\n",
    "    log_files = list(local_log_path.glob(\"*\"))\n",
    "    # get the latest log file and upload it, avoid upload irrelevant log files\n",
    "    # st_mtime represents the time of the last modification of the file in seconds\n",
    "    latest_log_file = max(log_files, key=lambda f: f.stat().st_mtime)\n",
    "    s3_upload_path = s3_output_base_path / \"log\" / latest_log_file.name\n",
    "    try:\n",
    "        s3_upload_path.upload_from(str(latest_log_file))\n",
    "        return Ok(f\"{latest_log_file} uploaded to {s3_output_base_path}\")\n",
    "    except Exception as e:\n",
    "        return Err(f\"Error uploading {latest_log_file} to {s3_output_base_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_variable_from_config_file(config) -> list and str and S3Path and Path:\n",
    "    target_pair_list = config[\"target_pair_list\"]\n",
    "    target_interval_list = config[\"target_interval_list\"]\n",
    "    tradingview_data_fetching_job_config = config[\"tradingview_data_fetching_job\"]\n",
    "    job_name = tradingview_data_fetching_job_config[\"job_name\"]\n",
    "    s3_output_base_path = S3Path(\n",
    "        tradingview_data_fetching_job_config[\"s3_output_base_path\"]\n",
    "    )\n",
    "    local_data_base_path = Path(\n",
    "        tradingview_data_fetching_job_config[\"local_data_base_path\"]\n",
    "    )\n",
    "    return (\n",
    "        target_pair_list,\n",
    "        target_interval_list,\n",
    "        job_name,\n",
    "        s3_output_base_path,\n",
    "        local_data_base_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tradingview_data_fetching_job(config_file_name_str: str) -> Ok[str] | Err[str]:\n",
    "    with open(f\"./config/{config_file_name_str}\", \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    # read variable from config file\n",
    "    (\n",
    "        target_pair_list,\n",
    "        target_interval_list,\n",
    "        job_name,\n",
    "        s3_output_base_path,\n",
    "        local_data_base_path,\n",
    "    ) = read_variable_from_config_file(config)\n",
    "    logger.info(f\"The name of this job: {job_name}\")\n",
    "    logger.info(f\"Configuration file name is {config_file_name_str}\")\n",
    "    logger.info(f\"Local TradingView data path is ./{local_data_base_path}\")\n",
    "    logger.info(f\"S3 TradingView data path is {s3_output_base_path}\")\n",
    "\n",
    "    # config logger object\n",
    "    local_log_path_name = f\"{job_name}_logs\"\n",
    "    local_log_path = Path(local_log_path_name)\n",
    "    create_local_logger(local_log_path_name, job_name)\n",
    "    logger.info(f\"The local log path is ./{local_log_path}\")\n",
    "\n",
    "    # config pair\n",
    "    config_pair_result = config_pair(target_pair_list)\n",
    "    if is_err(config_pair_result):\n",
    "        logger.error(config_pair_result.err_value)\n",
    "        return Err(config_pair_result.err_value)\n",
    "    target_pair_list = config_pair_result.ok_value\n",
    "    logger.success(f\"The target pair list for this job is: {target_pair_list}\")\n",
    "\n",
    "    # config time interval\n",
    "    config_interval_result = config_interval(target_interval_list)\n",
    "    if is_err(config_interval_result):\n",
    "        logger.error(config_interval_result.err_value)\n",
    "        return Err(config_interval_result.err_value)\n",
    "    target_interval_list = config_interval_result.ok_value\n",
    "    logger.success(f\"The target interval list for this job is: {target_interval_list}\")\n",
    "\n",
    "    # generate related datetime for naming the file\n",
    "    datetime_handler_result = datetime_handler(target_interval_list)\n",
    "    if is_err(datetime_handler_result):\n",
    "        logger.error(datetime_handler_result.err_value)\n",
    "        return Err(datetime_handler_result.err_value)\n",
    "    datetime_dict = datetime_handler_result.ok_value\n",
    "    logger.success(f\"The target datetime for this job is: {datetime_dict}\")\n",
    "\n",
    "    # delete all data before data fetching if old data have not been deleted\n",
    "    if local_data_base_path.exists():\n",
    "        logger.info(\"Cleared the tradingview data folder before new data comes in\")\n",
    "        delete_local_data(local_data_base_path)\n",
    "\n",
    "    # fetch technical analysis summary for each pair and each time interval\n",
    "    logger.info(\n",
    "        \"Start fetching technical analysis summary for each pair and each time interval\"\n",
    "    )\n",
    "    fetch_technical_analysis_summary_result = fetch_technical_analysis_summary(\n",
    "        \"forex\", target_interval_list, target_pair_list\n",
    "    )\n",
    "    if is_err(fetch_technical_analysis_summary_result):\n",
    "        logger.error(fetch_technical_analysis_summary_result.err_value)\n",
    "        upload_latest_log_to_s3(local_log_path, s3_output_base_path)\n",
    "        return Err(fetch_technical_analysis_summary_result.err_value)\n",
    "    logger.success(\"Fetched all target technical analysis summary\")\n",
    "    technical_analysis_summary_dict = fetch_technical_analysis_summary_result.ok_value\n",
    "\n",
    "    # fetch technical indicators for each pair and each time interval\n",
    "    logger.info(\n",
    "        \"Start fetching technical indicators for each pair and each time interval\"\n",
    "    )\n",
    "    fetch_technical_indicators_result = fetch_technical_indicators(\n",
    "        \"forex\", target_interval_list, target_pair_list\n",
    "    )\n",
    "    if is_err(fetch_technical_indicators_result):\n",
    "        logger.error(fetch_technical_indicators_result.err_value)\n",
    "        upload_latest_log_to_s3(local_log_path, s3_output_base_path)\n",
    "        return Err(fetch_technical_indicators_result.err_value)\n",
    "    logger.success(\"Fetched all target technical indicators\")\n",
    "    technical_indicators_dict = fetch_technical_indicators_result.ok_value\n",
    "\n",
    "    # combined these two dictionaries\n",
    "    combined_dict = get_combined_dict(\n",
    "        technical_analysis_summary_dict, technical_indicators_dict\n",
    "    )\n",
    "\n",
    "    # for the value of each pair and each time interval in the combined dict, save it as json file to local folder\n",
    "    logger.info(\n",
    "        \"For the value of each pair and each time interval in the combined dict, start transforming it into json file\"\n",
    "    )\n",
    "    save_dict_as_json_to_local_file_result = save_dict_as_json_to_local_file(\n",
    "        combined_dict, local_data_base_path, datetime_dict\n",
    "    )\n",
    "    if is_err(save_dict_as_json_to_local_file_result):\n",
    "        logger.error(save_dict_as_json_to_local_file_result.err_value)\n",
    "        upload_latest_log_to_s3(local_log_path, s3_output_base_path)\n",
    "        return Err(save_dict_as_json_to_local_file_result.err_value)\n",
    "    logger.success(save_dict_as_json_to_local_file_result.ok_value)\n",
    "\n",
    "    # upload data to S3\n",
    "    logger.info(\"Start uploading data from local to S3 bucket\")\n",
    "    upload_local_data_to_s3_result = upload_local_data_to_s3(\n",
    "        target_pair_list,\n",
    "        local_data_base_path,\n",
    "        s3_output_base_path,\n",
    "    )\n",
    "    if is_err(upload_local_data_to_s3_result):\n",
    "        logger.error(upload_local_data_to_s3_result.err_value)\n",
    "        return Err(upload_local_data_to_s3_result.err_value)\n",
    "    logger.success(upload_local_data_to_s3_result.ok_value)\n",
    "\n",
    "    # upload log to S3\n",
    "    logger.info(\"Start uploading current log file from local to S3 bucket\")\n",
    "    upload_latest_log_to_s3_result = upload_latest_log_to_s3(\n",
    "        local_log_path, s3_output_base_path\n",
    "    )\n",
    "    if is_err(upload_latest_log_to_s3_result):\n",
    "        logger.error(upload_latest_log_to_s3_result.err_value)\n",
    "        return Err(upload_latest_log_to_s3_result.err_value)\n",
    "    logger.success(upload_latest_log_to_s3_result.ok_value)\n",
    "    logger.success(\"Finish TradingView data fetching job\")\n",
    "\n",
    "    # delete all data\n",
    "    delete_all_local_data(local_data_base_path, local_log_path)\n",
    "\n",
    "    return Ok(f\"Finish TradingView data fetching job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradingview_data_fetching_job(config_file_name_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
